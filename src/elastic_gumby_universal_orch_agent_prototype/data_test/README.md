# Test Cases

This directory contains test cases for the Universal Orchestration Agent. Each test case demonstrates how the agent can translate natural language workflow descriptions into structured, executable workflows following the schema defined in `/data_schema/workflow_schema.json`.

## File Types and Structure

The test files follow a consistent naming pattern with different suffixes indicating their purpose:

### Input Files (Agent Inputs)
- **`reference_*_desc.json`** - Natural language workflow descriptions that serve as input to the Universal Orchestration Agent
- **`reference_*_tools.json`** - Available tools and their specifications that the agent can use when generating workflows

### Reference Files (Expected Outputs)
- **`reference_*_workflow.json`** - Human-reviewed expected workflow plans that serve as the ground truth for evaluating agent performance

### Generated Files (Agent Outputs)
- **`generated_*_workflow.json`** - Workflow plans generated by the Universal Orchestration Agent based on the corresponding input files

## Evaluation

The `generated_*_workflow.json` and corresponding `reference_*_workflow.json` files can be used as inputs to the `compare_workflow(generated_workflow, reference_workflow)` method in `/metrics/utils.py` to obtain plan evaluation results. This comparison method evaluates the quality and accuracy of the agent-generated workflows against the human-reviewed reference workflows.

## Test Cases

Each test case consists of a set of files with the same base name (e.g., "QT", "deep_research", "short", "long", "operation_monitoring_system") representing different workflow scenarios and complexity levels.
